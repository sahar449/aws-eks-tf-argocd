name: CI-CD Full Pipeline

on:
  workflow_dispatch:
    inputs:
      action:
        description: 'Choose action'
        required: true
        default: 'apply'
        type: choice
        options:
          - apply
          - destroy

permissions:
  id-token: write
  contents: read

env:
  aws_region: us-west-2
  terraform_version: 1.5.7
  image_tag: latest
  repo_name: eksdemo

jobs:
  ci_cd_pipeline:
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.action == 'apply' }}

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: ${{ env.terraform_version }}

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v3
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ env.aws_region }}
          role-session-name: github-actions

      - name: Terraform Apply ECR
        run: |
          terraform init -upgrade
          terraform apply -auto-approve -target=module.ecr

      - name: Login to ECR
        run: |
          aws ecr get-login-password --region ${{ env.aws_region }} | docker login --username AWS --password-stdin ${{ secrets.AWS_ACCOUNT_NUMBER }}.dkr.ecr.${{ env.aws_region }}.amazonaws.com

      - name: Build Docker Image
        run: |
          docker build -t ${{ secrets.AWS_ACCOUNT_NUMBER }}.dkr.ecr.${{ env.aws_region }}.amazonaws.com/${{ env.repo_name }}:${{ env.image_tag }} ./python-docker

      - name: Push Docker Image to ECR
        run: |
          docker push ${{ secrets.AWS_ACCOUNT_NUMBER }}.dkr.ecr.${{ env.aws_region }}.amazonaws.com/${{ env.repo_name }}:${{ env.image_tag }}

      - name: Terraform Apply Infrastructure
        run: |
          terraform apply -auto-approve

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --region ${{ env.aws_region }} --name eksdemo-cluster

      - name: Patch aws-auth ConfigMap
        run: |
          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: aws-auth
            namespace: kube-system
          data:
            mapUsers: |
              - userarn: arn:aws:iam::${{ secrets.AWS_ACCOUNT_NUMBER }}:user/tf
                username: tf
                groups:
                  - system:masters
          EOF
      
      - name: Install ALB Controller via Helm
        run: |
          helm repo add eks https://aws.github.io/eks-charts
          helm repo update
          helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller \
            --namespace kube-system \
            --create-namespace \
            --set clusterName=${{ env.cluster_name }} \
            --set region=${{ env.aws_region }} \
            --set vpcId=$VPC_ID \
            --set serviceAccount.create=true \
            --set serviceAccount.name=aws-load-balancer-controller \
            --set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"="arn:aws:iam::${{ secrets.AWS_ACCOUNT_NUMBER }}:role/$ALB_ROLE_NAME" \
            --set cleanupOnDelete=true

      - name: Install ExternalDNS via Helm
        run: |
          helm repo add external-dns https://kubernetes-sigs.github.io/external-dns/
          helm repo update
          helm upgrade --install external-dns external-dns/external-dns \
            --namespace kube-system \
            --create-namespace \
            --set provider=aws \
            --set region=${{ env.aws_region }} \
            --set domainFilters[0]=saharbittman.com \
            --set policy=sync \
            --set sources[0]=ingress \
            --set sources[1]=service \
            --set serviceAccount.create=true \
            --set serviceAccount.name=external-dns \
            --set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"="arn:aws:iam::${{ secrets.AWS_ACCOUNT_NUMBER }}:role/$DNS_ROLE_NAME" \
            --set txtOwnerId=${{ env.cluster_name }} \
            --set cleanupOnDelete=true

      - name: Helm Install ArgoCD
        run: |
          helm repo add argo https://argoproj.github.io/argo-helm
          helm repo update
          helm upgrade --install argocd argo/argo-cd \
            --namespace argocd \
            --create-namespace \
            --set server.service.type=LoadBalancer \
            --set server.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-type"="classic"

      - name: Clone repo and apply AppOfApps
        run: |
          git clone https://github.com/sahar449/aws-eks-tf-argocd.git
          sed -i "s|repository: .*|repository: ${AWS_ACCOUNT_NUMBER}.dkr.ecr.${AWS_REGION}.amazonaws.com/${REPO_NAME}|" ./chart-flask/values.yaml
          cd aws-eks-tf-argocd/argocd-apps/argocd-flask
          kubectl apply -f flask-app.yaml

  terraform_destroy:
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.action == 'destroy' || failure() }}
    needs: ci_cd_pipeline

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: ${{ env.terraform_version }}

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v3
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ env.aws_region }}
          role-session-name: github-actions

      - name: Destroy EKS
        run: |
          aws eks update-kubeconfig --region ${{ env.aws_region }} --name eksdemo-cluster
          # git clone https://github.com/sahar449/aws-eks-tf-argocd.git
          # cd aws-eks-tf-argocd/argocd-apps/argocd-flask
          # kubectl patch app flask-app -n argocd -p '{"metadata":{"finalizers":null}}' --type=merge
          kubectl delete -f flask-app.yaml
          helm uninstall argocd -n argocd || true
          terraform init -upgrade
          terraform destroy -target=module.eks -auto-approve || true

      - name: Remove Security Groups from VPC
        run: |
          for sg in $(aws ec2 describe-security-groups --region ${{ env.aws_region }} \
            --query 'SecurityGroups[?GroupName!=`default`].GroupId' --output text); do
            echo "Trying to delete SG $sg"
            aws ec2 delete-security-group --region ${{ env.aws_region }} --group-id $sg || echo "Cannot delete $sg, still in use"
          done


      - name: Terraform Destroy Remaining Resources
        run: |
          terraform init -upgrade
          terraform destroy -auto-approve || true
